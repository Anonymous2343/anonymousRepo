# anonymousRepo
AnonymousRepo
Since developing and evaluating neural decompilers is an active research field, we plan to open-source the framework after acceptance.
We ask the reviewers and PC for consent, but provide verifiable results.

Please refer to the "Verifiability" Section 5.3.3.

We provide verifiable results for graphs and tables.
We structure the repository according to the papers structure.

There are 4 kind of files:
 1. function_logs.jsonl (generated by the framework)
 2. edit_distances.txt (generated by the framework)
 3. output.txt (generated by the framework)
 4. Python helper scripts to verify the results (generated by ChatGPT for our reviewers)
### Now each file in more detail:
1. function_logs.jsonl:
   This file contains N=116, N=380, and N>=1500 functions, their respective decompilation, and the compilability and pass flags. Statistics are always averaged over all functions.
2. edit_distance.txt:
   This file contains multiple columns, first and second column are compile and pass flags, other columns are information about code similarity metrics, GCOV coverage and KLEE coverage.
3. output.txt:
   Protocols each step that the framework has done. From source file and program collection to decompilation, and KLEE test generation.
5. Statistics.py:
   This file takes a local edit_distances.txt as input, and calculates various statistics.

   hamming_distance.py:
   This script is a TK-inter based GUI, which takes two function_logs.jsonl files per drag-and-drop as input. It calculates hamming distances, and yields information about mismatches with respect to compilability and pass flags.
   Reviewers can most often use this script to get statistics.

   Table 3 specific:
   match_function_jsonl.py:
   In Table 3 we evaluated for both decompilers more than 2500 functions. We noticed that not both function_logs.jsonl target the same functions troughoutly. Therefore, we execute first the match_functions_jsonl.py to get new cleaned *.jsonl files, which will contain the N=1500 functions addressed in the paper. After renaming for example edit_distances_a_O0.txt.cleaned -> edit_distances.txt and run Statistics.py, it will yield compile, pass ratio, and coverage information for this particular case.
   Here "a" stands for ANGR, and "l" for LLM4Decompile.

   code_similarity.py:
   This file takes function_logs.jsonl and edit_distances.txt as input, and runs various code similarity metrices. Might require installing according python libraries.

   
   
